{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "魔数:2051, 图片数量: 60000张, 图片大小: 28*28\n",
      "已解析 10000张\n",
      "已解析 20000张\n",
      "已解析 30000张\n",
      "已解析 40000张\n",
      "已解析 50000张\n",
      "已解析 60000张\n",
      "魔数:2051, 图片数量: 10000张, 图片大小: 28*28\n",
      "已解析 10000张\n",
      "魔数:2049, 图片数量: 60000张\n",
      "已解析 10000张\n",
      "已解析 20000张\n",
      "已解析 30000张\n",
      "已解析 40000张\n",
      "已解析 50000张\n",
      "已解析 60000张\n",
      "魔数:2049, 图片数量: 10000张\n",
      "已解析 10000张\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练集文件\n",
    "train_images_idx3_ubyte_file = '../data/common/train-images-idx3-ubyte'\n",
    "# 训练集标签文件\n",
    "train_labels_idx1_ubyte_file = '../data/common/train-labels-idx1-ubyte'\n",
    "\n",
    "# 测试集文件\n",
    "test_images_idx3_ubyte_file = '../data/common/t10k-images-idx3-ubyte'\n",
    "# 测试集标签文件\n",
    "test_labels_idx1_ubyte_file = '../data/common/t10k-labels-idx1-ubyte'\n",
    "def decode_idx3_ubyte(idx3_ubyte_file):\n",
    "    \"\"\"\n",
    "    解析idx3文件的通用函数\n",
    "    :param idx3_ubyte_file: idx3文件路径\n",
    "    :return: 数据集\n",
    "    \"\"\"\n",
    "    # 读取二进制数据\n",
    "    bin_data = open(idx3_ubyte_file, 'rb').read()\n",
    "\n",
    "    # 解析文件头信息，依次为魔数、图片数量、每张图片高、每张图片宽\n",
    "    offset = 0\n",
    "    fmt_header = '>iiii'\n",
    "    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('魔数:%d, 图片数量: %d张, 图片大小: %d*%d' % (magic_number, num_images, num_rows, num_cols))\n",
    "\n",
    "    # 解析数据集\n",
    "    image_size = num_rows * num_cols\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>' + str(image_size) + 'B'\n",
    "    images = np.empty((num_images, num_rows, num_cols))\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('已解析 %d' % (i + 1) + '张')\n",
    "        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols))\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def decode_idx1_ubyte(idx1_ubyte_file):\n",
    "    \"\"\"\n",
    "    解析idx1文件的通用函数\n",
    "    :param idx1_ubyte_file: idx1文件路径\n",
    "    :return: 数据集\n",
    "    \"\"\"\n",
    "    # 读取二进制数据\n",
    "    bin_data = open(idx1_ubyte_file, 'rb').read()\n",
    "\n",
    "    # 解析文件头信息，依次为魔数和标签数\n",
    "    offset = 0\n",
    "    fmt_header = '>ii'\n",
    "    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('魔数:%d, 图片数量: %d张' % (magic_number, num_images))\n",
    "\n",
    "    # 解析数据集\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>B'\n",
    "    labels = np.empty(num_images)\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('已解析 %d' % (i + 1) + '张')\n",
    "        labels[i] = struct.unpack_from(fmt_image, bin_data, offset)[0]\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return labels\n",
    "train_image=decode_idx3_ubyte(train_images_idx3_ubyte_file)\n",
    "test_image=decode_idx3_ubyte(test_images_idx3_ubyte_file)\n",
    "train_label=decode_idx1_ubyte(train_labels_idx1_ubyte_file)\n",
    "test_label=decode_idx1_ubyte(test_labels_idx1_ubyte_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onezero_size(labels):\n",
    "    size=labels.shape[0]\n",
    "    j=0\n",
    "    for i in range(size):\n",
    "        if (labels[i]==0)or(labels[i]==1):\n",
    "            j+=1\n",
    "    return j\n",
    "train_num=get_onezero_size(train_label)\n",
    "test_num=get_onezero_size(train_label)\n",
    "def get_onezeros_sample(num,images,labels):\n",
    "    images1=np.empty((num,28,28))\n",
    "    labels1=np.empty(num)\n",
    "    size=labels.shape[0]\n",
    "    j=0\n",
    "    for i in range(size):\n",
    "        if (labels[i]==0)or(labels[i]==1):\n",
    "            images1[j]=images[i]\n",
    "            labels1[j]=labels[i]\n",
    "            j+=1\n",
    "    return images1,labels1\n",
    "train_images,train_labels=get_onezeros_sample(train_num,train_image,train_label)\n",
    "test_images,test_labels=get_onezeros_sample(test_num,test_image,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex_logi_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions [f,g] = logistic_regression_vec(theta, X,y)\n",
    "#   %\n",
    "#   % Arguments:\n",
    "#   %   theta - A column vector containing the parameter values to optimize.\n",
    "#   %   X - The examples stored in a matrix.  \n",
    "#   %       X(i,j) is the i'th coordinate of the j'th example.\n",
    "#   %   y - The label for each example.  y(j) is the j'th example's label.\n",
    "#   %\n",
    "def log_reg(theta,X,y):\n",
    "    m=X.shape[1];\n",
    "    n=X.shape[0];\n",
    "  #   % initialize objective value and gradient.\n",
    "    f = 0;\n",
    "    g=np.zeros(theta.shape[1])\n",
    "    temp=np.dot(theta,X)\n",
    "    print(\"temp,\",temp)\n",
    "    h_temp=1/(1+np.exp(-temp))\n",
    "    print(\"h_temp\",h_temp)\n",
    "    cost=y*np.log(h_temp)+(1-y)*np.log(1-h_temp)\n",
    "    f=-np.sum(cost)\n",
    "    print(\"cost:\",f)\n",
    "    for j in range(0,n): \n",
    "        \n",
    "        g[j]=np.sum((h_temp-y)*X[j,:])\n",
    "    return f,g\n",
    "\n",
    "#TODO:  Compute the logistic regression objective function and gradient \n",
    "#   %        using vectorized code.  (It will be just a few lines of code!)\n",
    "#   %        Store the objective function value in 'f', and the gradient in 'g'.\n",
    "#   %\n",
    "# %%% YOUR CODE HERE %%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 12665)\n"
     ]
    }
   ],
   "source": [
    "train_X=np.empty((784,train_num))\n",
    "test_X=np.empty((784,test_num))\n",
    "for i in range(10):\n",
    "    train_X[:,i]=train_images[i].reshape(784)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Load the MNIST data for this exercise.\n",
    "# % train.X and test.X will contain the training and testing images.\n",
    "# %   Each matrix has size [n,m] where:\n",
    "# %      m is the number of examples.\n",
    "# %      n is the number of pixels in each image.\n",
    "# % train.y and test.y will contain the corresponding labels (0 or 1).\n",
    "train_X=np.empty((784,train_num))\n",
    "test_X=np.empty((784,test_num))\n",
    "for i in range(train_num):\n",
    "    train_X[:,i]=train_images[i].reshape(784)\n",
    "for i in range(test_num):\n",
    "    test_X[:,i]=test_images[i].reshape(784)\n",
    "cols1=train_X.shape[1]\n",
    "append1=np.ones(shape=(1,cols1))\n",
    "cols2=test_X.shape[1]\n",
    "append2=np.ones(shape=(1,cols2))\n",
    "# % Include a row of 1s as an additional intercept feature.\n",
    "# % Add row of 1s to the dataset to act as an intercept term.\n",
    "train_X = np.insert(train_X,0,values=append1,axis=0)\n",
    "test_X = np.insert(test_X,0,values=append2,axis=0)\n",
    "\n",
    "#% Training set dimensions\n",
    "m=train_X.shape[1]\n",
    "n=train_X.shape[0]\n",
    "\n",
    "#% Train logistic regression classifier using minFunc\n",
    "#options = struct('MaxIter', 100);\n",
    "\n",
    "#% First, we initialize theta to some small random values.\n",
    "#theta = np.random.random(n).reshape(1,n);\n",
    "theta = init_weights(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=train_labels\n",
    "test_y=test_labels\n",
    "def ex_logistic_reg(theta,X,y,lr,epoches):\n",
    "    res_theta=theta\n",
    "    costmin=1000000\n",
    "    for i in range(epoches):\n",
    "        f,g=log_reg(theta,X,y)\n",
    "        print(f)\n",
    "        print(\"the cost is %f\",f)\n",
    "        theta-=lr*g\n",
    "        if costmin>f:\n",
    "            costmin=f\n",
    "            res_theta=theta\n",
    "    return res_theta\n",
    "lr=0.5\n",
    "epoches=1000\n",
    "costmin=1000   \n",
    "res_theta=ex_logistic_reg(theta,train_X,train_y,lr,epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: %2.1f%%\n",
      " 99.9289380182\n",
      "[[False  True  True ...,  True False  True]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def binary_classifier_accuracy(theta,X,y):\n",
    "    res=np.dot(theta,X)\n",
    "    h_res=1/(1+np.exp(-res))\n",
    "    predicted_y=h_res>0.5\n",
    "    lens=len(y)\n",
    "    acu=np.sum(predicted_y==y)/lens\n",
    "    return acu,predicted_y\n",
    "accuracy ,predicted_y= binary_classifier_accuracy(res_theta,train_X,train_y);\n",
    "print('Training accuracy: %2.1f%%\\n', 100*accuracy);\n",
    "print(predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: %2.1f%%\n",
      " 16.6837741808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "accuracy ,predicted_y= binary_classifier_accuracy(res_theta,test_X,test_y);\n",
    "print('Training accuracy: %2.1f%%\\n', 100*accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Load the MNIST data for this exercise.\n",
    "# % train.X and test.X will contain the training and testing images.\n",
    "# %   Each matrix has size [n,m] where:\n",
    "# %      m is the number of examples.\n",
    "# %      n is the number of pixels in each image.\n",
    "# % train.y and test.y will contain the corresponding labels (0 or 1).\n",
    "train_X=np.empty((784,train_num))\n",
    "test_X=np.empty((784,test_num))\n",
    "for i in range(train_num):\n",
    "    train_X[:,i]=train_images[i].reshape(784)\n",
    "for i in range(test_num):\n",
    "    test_X[:,i]=test_images[i].reshape(784)\n",
    "cols1=train_X.shape[1]\n",
    "append1=np.ones(shape=(1,cols1))\n",
    "cols2=test_X.shape[1]\n",
    "append2=np.ones(shape=(1,cols2))\n",
    "# % Include a row of 1s as an additional intercept feature.\n",
    "# % Add row of 1s to the dataset to act as an intercept term.\n",
    "train_X = np.insert(train_X,0,values=append1)\n",
    "test_X = np.insert(test_X,0,values=append2)\n",
    "train_y=train_labels\n",
    "test_y=test_labels\n",
    "#% Training set dimensions\n",
    "m=train_X.shape[1]\n",
    "n=train_X.shape[0]\n",
    "\n",
    "#% Train logistic regression classifier using minFunc\n",
    "#options = struct('MaxIter', 100);\n",
    "\n",
    "#% First, we initialize theta to some small random values.\n",
    "theta = np.random.randn(1,n)*0.01;\n",
    "\n",
    "# % Call minFunc with the logistic_regression.m file as the objective function.\n",
    "# %\n",
    "# % TODO:  Implement batch logistic regression in the logistic_regression.m file!\n",
    "# %\n",
    "# tic;\n",
    "# theta=minFunc(@logistic_regression, theta, options, train.X, train.y);\n",
    "# fprintf('Optimization took %f seconds.\\n', toc);\n",
    "\n",
    "# % Now, call minFunc again with logistic_regression_vec.m as objective.\n",
    "# %\n",
    "# % TODO:  Implement batch logistic regression in logistic_regression_vec.m using\n",
    "# % MATLAB's vectorization features to speed up your code.  Compare the running\n",
    "# % time for your logistic_regression.m and logistic_regression_vec.m implementations.\n",
    "# %\n",
    "def init_weights(n_features):\n",
    "    # 初始化参数\n",
    "    # 参数范围(-1/sqrt(N), 1/sqrt(N))\n",
    "    limit = np.sqrt(1/n_features)\n",
    "    w = np.random.uniform(-limit, limit, (1,n_features))\n",
    "    return w\n",
    "def ex_logistic_reg(theta,X,y,lr,epoches):\n",
    "    res_theta=theta\n",
    "    for i in range(epoches):\n",
    "        [f,g]=log_reg(theta,X,y)\n",
    "        print(\"the cost is %f\",f)\n",
    "        theta-=lr*g\n",
    "        if costmin>f:\n",
    "            costmin=f\n",
    "            res_theta=theta\n",
    "    return res_theta\n",
    "lr=0.05\n",
    "epoches=1000\n",
    "costmin=1000   \n",
    "res_theta=ex_logistic_reg(theta,train_X,train_y,lr,epoches)\n",
    "# % Uncomment the lines below to run your vectorized code.\n",
    "# %theta = rand(n,1)*0.001;\n",
    "# %tic;\n",
    "# %theta=minFunc(@logistic_regression_vec, theta, options, train.X, train.y);\n",
    "# %fprintf('Optimization took %f seconds.\\n', toc);\n",
    "\n",
    "# % Print out training accuracy.\n",
    "def binary_classifier_accuracy(theta,X,y):\n",
    "    res=np.dot(theta,X)\n",
    "    h_res=1/(1+np.exp(-res))\n",
    "    predicted_y=h_res>0.5\n",
    "    lens=len(y)\n",
    "    acu=np.sum(predicted_y==y)/lens\n",
    "    return acu\n",
    "accuracy = binary_classifier_accuracy(res_theta,train_X,train_y);\n",
    "print('Training accuracy: %2.1f%%\\n', 100*accuracy);\n",
    "\n",
    "#% Print out accuracy on the test set.\n",
    "accuracy = binary_classifier_accuracy(res_theta,test_X,test_y);\n",
    "print('Test accuracy: %2.1f%%\\n', 100*accuracy);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "# n:特征数，k:标签数\n",
    "n = X.shape[1]\n",
    "label = np.unique(y)\n",
    "k = len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "X = data.data[data.target != 0]/np.linalg.norm(data.data[data.target != 0], axis=0)\n",
    "y = data.target[data.target != 0]\n",
    "y[y == 1] = 0\n",
    "y[y == 2] = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_weights(n_features):\n",
    "    # 初始化参数\n",
    "    # 参数范围(-1/sqrt(N), 1/sqrt(N))\n",
    "    limit = np.sqrt(1/n_features)\n",
    "    w = np.random.uniform(-limit, limit, (1,n_features))\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_logistic_reg(theta,X,y,lr,epoches):\n",
    "    res_theta=theta\n",
    "    costmin=10000\n",
    "    for i in range(epoches):\n",
    "        [f,g]=log_reg(theta,X,y)\n",
    "        print(\"the cost is %f\"%f)\n",
    "        theta-=lr*g\n",
    "        if costmin>f:\n",
    "            costmin=f\n",
    "            res_theta=theta\n",
    "    return res_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 75) (5, 15)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=0.75, test_size=0.15, random_state=1)\n",
    "X_train=X_train.T\n",
    "X_train=np.insert(X_train, 0, 1, axis=0)  # 添加一行\n",
    "X_test=X_test.T\n",
    "X_test=np.insert(X_test, 0, 1, axis=0)  # 添加一行\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 75)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=X_train.shape[1]\n",
    "n=X_train.shape[0]\n",
    "theta = init_weights(n)\n",
    "print(theta)\n",
    "epoches=1000\n",
    "res_theta=ex_logistic_reg(theta,X_train,Y_train,lr,epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = binary_classifier_accuracy(res_theta,X_train,Y_train);\n",
    "print('Training accuracy: %2.1f%\\n'%100*accuracy);\n",
    "\n",
    "#% Print out accuracy on the test set.\n",
    "accuracy = binary_classifier_accuracy(res_theta,X_test,Y_test);\n",
    "print('Test accuracy: %2.1f%\\n'%100*accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(theta,X,y):\n",
    "    m=X.shape[1];\n",
    "    n=X.shape[0];\n",
    "  #   % initialize objective value and gradient.\n",
    "    f = 0;\n",
    "    g=np.zeros(theta.shape[1])\n",
    "    temp=np.dot(theta,X)\n",
    "    print(\"temp,\",temp)\n",
    "    h_temp=1/(1+np.exp(-temp))\n",
    "    print(\"h_temp\",h_temp)\n",
    "    cost=y*np.log(h_temp)+(1-y)*np.log(1-h_temp)\n",
    "    f=-np.sum(cost)\n",
    "    return f\n",
    "def gredient_check(f,g,theta,X,y):\n",
    "    epsilon = 1e-4\n",
    "    checked=[]\n",
    "    for i in range(len(theta)):\n",
    "        theta1=theta\n",
    "        theta2=theta\n",
    "        theta1[i]-=epsilon\n",
    "        theta2[i]+=epsilon\n",
    "        check=(cost_func(theta2,X,y)-cost_func(theta1,X,y))/2*epsilon\n",
    "        checked.append(check)\n",
    "        print(\"gredient of %d is\"%i,g[i])\n",
    "        print(\"checked_gredient of %d is\"%i,check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
