{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "魔数:2051, 图片数量: 60000张, 图片大小: 28*28\n",
      "已解析 10000张\n",
      "已解析 20000张\n",
      "已解析 30000张\n",
      "已解析 40000张\n",
      "已解析 50000张\n",
      "已解析 60000张\n",
      "魔数:2051, 图片数量: 10000张, 图片大小: 28*28\n",
      "已解析 10000张\n",
      "魔数:2049, 图片数量: 60000张\n",
      "已解析 10000张\n",
      "已解析 20000张\n",
      "已解析 30000张\n",
      "已解析 40000张\n",
      "已解析 50000张\n",
      "已解析 60000张\n",
      "魔数:2049, 图片数量: 10000张\n",
      "已解析 10000张\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练集文件\n",
    "train_images_idx3_ubyte_file = '../data/common/train-images-idx3-ubyte'\n",
    "# 训练集标签文件\n",
    "train_labels_idx1_ubyte_file = '../data/common/train-labels-idx1-ubyte'\n",
    "\n",
    "# 测试集文件\n",
    "test_images_idx3_ubyte_file = '../data/common/t10k-images-idx3-ubyte'\n",
    "# 测试集标签文件\n",
    "test_labels_idx1_ubyte_file = '../data/common/t10k-labels-idx1-ubyte'\n",
    "def decode_idx3_ubyte(idx3_ubyte_file):\n",
    "    \"\"\"\n",
    "    解析idx3文件的通用函数\n",
    "    :param idx3_ubyte_file: idx3文件路径\n",
    "    :return: 数据集\n",
    "    \"\"\"\n",
    "    # 读取二进制数据\n",
    "    bin_data = open(idx3_ubyte_file, 'rb').read()\n",
    "\n",
    "    # 解析文件头信息，依次为魔数、图片数量、每张图片高、每张图片宽\n",
    "    offset = 0\n",
    "    fmt_header = '>iiii'\n",
    "    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('魔数:%d, 图片数量: %d张, 图片大小: %d*%d' % (magic_number, num_images, num_rows, num_cols))\n",
    "\n",
    "    # 解析数据集\n",
    "    image_size = num_rows * num_cols\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>' + str(image_size) + 'B'\n",
    "    images = np.empty((num_images, num_rows, num_cols))\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('已解析 %d' % (i + 1) + '张')\n",
    "        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols))\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def decode_idx1_ubyte(idx1_ubyte_file):\n",
    "    \"\"\"\n",
    "    解析idx1文件的通用函数\n",
    "    :param idx1_ubyte_file: idx1文件路径\n",
    "    :return: 数据集\n",
    "    \"\"\"\n",
    "    # 读取二进制数据\n",
    "    bin_data = open(idx1_ubyte_file, 'rb').read()\n",
    "\n",
    "    # 解析文件头信息，依次为魔数和标签数\n",
    "    offset = 0\n",
    "    fmt_header = '>ii'\n",
    "    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('魔数:%d, 图片数量: %d张' % (magic_number, num_images))\n",
    "\n",
    "    # 解析数据集\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>B'\n",
    "    labels = np.empty(num_images)\n",
    "    for i in range(num_images):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('已解析 %d' % (i + 1) + '张')\n",
    "        labels[i] = struct.unpack_from(fmt_image, bin_data, offset)[0]\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return labels\n",
    "train_image=decode_idx3_ubyte(train_images_idx3_ubyte_file)\n",
    "test_image=decode_idx3_ubyte(test_images_idx3_ubyte_file)\n",
    "train_label=decode_idx1_ubyte(train_labels_idx1_ubyte_file)\n",
    "test_label=decode_idx1_ubyte(test_labels_idx1_ubyte_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=np.array(np.arange(10))\n",
    "cols=y\n",
    "rows=np.array([0,3,1,2,2,1,0,3,2,1])\n",
    "data=np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "coo_mat = sparse.coo_matrix((data, (rows, cols)), shape=(4, 10)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function [f,g] = softmax_regression(theta, X,y)\n",
    "#   %\n",
    "#   % Arguments:\n",
    "#   %   theta - A vector containing the parameter values to optimize.\n",
    "#   %       In minFunc, theta is reshaped to a long vector.  So we need to\n",
    "#   %       resize it to an n-by-(num_classes-1) matrix.\n",
    "#   %       Recall that we assume theta(:,num_classes) = 0.\n",
    "#   %\n",
    "#   %   X - The examples stored in a matrix.  \n",
    "#   %       X(i,j) is the i'th coordinate of the j'th example.\n",
    "#   %   y - The label for each example.  y(j) is the j'th example's label.\n",
    "#   %\n",
    "def softmax_reg(theta,X,y):\n",
    "    m=X.shape[1];\n",
    "    n=X.shape[0];\n",
    "  #   % initialize objective value and gradient.\n",
    "\n",
    "#   % theta is a vector;  need to reshape to n x num_classes.\n",
    "    #theta=reshape(theta, n, []);theta=theta.reshape()\n",
    "    num_classes=theta.shape[0]\n",
    "  #% initialize objective value and gradient.\n",
    "    f = 0;\n",
    "    g = np.zeros(shape=theta.shape)\n",
    "    re=np.dot(theta,X)\n",
    "    print(re)\n",
    "    re1=np.exp(re)\n",
    "    re2=np.sum(re1,axis=0)\n",
    "    re3=re1/re2\n",
    "    re4=np.log(re3)\n",
    "    cols=np.array(np.arange(m))\n",
    "    rows=y\n",
    "    data=np.array([1]*m)\n",
    "    coo_mat = sparse.coo_matrix((data, (rows, cols)), shape=(num_classes, m)).toarray()\n",
    "    re5=coo_mat*re4\n",
    "    f=-np.sum(re5)\n",
    "    g=np.dot((coo_mat-re3),X.T)\n",
    "    return f,g\n",
    "def cost_func(theta,X,y):\n",
    "    m=X.shape[1];\n",
    "    n=X.shape[0];\n",
    "    num_classes=theta.shape[0]\n",
    "    f = 0;\n",
    "    g = np.zeros(shape=theta.shape)\n",
    "    re=np.dot(theta,X)\n",
    "    re1=np.exp(re)\n",
    "    re2=np.sum(re1,axis=0)\n",
    "    re3=re1/re2\n",
    "    re4=np.log(re3)\n",
    "    cols=np.array(np.arange(m))\n",
    "    rows=y\n",
    "    data=np.array([1]*m)\n",
    "    coo_mat = sparse.coo_matrix((data, (rows, cols)), shape=(num_classes, m)).toarray()\n",
    "    re5=coo_mat*re4\n",
    "    f=-np.sum(re5)\n",
    "    return f\n",
    "def gredient_check(g,theta,k,X,y):\n",
    "    epsilon = 1e-4\n",
    "    checked=[]\n",
    "    n=theta.shape[1]\n",
    "    for i in range(n):\n",
    "        theta1=theta\n",
    "        theta2=theta\n",
    "        theta1[k][i]-=epsilon\n",
    "        theta2[k][i]+=epsilon\n",
    "        print(\"cost1=\",cost_func(theta1,X,y))\n",
    "        print(\"cost2=\",cost_func(theta2,X,y))\n",
    "        check=(cost_func(theta2,X,y)-cost_func(theta1,X,y))/2*epsilon\n",
    "        checked.append(check)\n",
    "        print(\"gredient of %d is\"%i,g[k][i])\n",
    "        print(\"checked_gredient of %d is\"%i,check)\n",
    "    #   %\n",
    "#   % TODO:  Compute the softmax objective function and gradient using vectorized code.\n",
    "#   %        Store the objective function value in 'f', and the gradient in 'g'.\n",
    "#   %        Before returning g, make sure you form it back into a vector with g=g(:);\n",
    "#   %\n",
    "# %%% YOUR CODE HERE %%%\n",
    "  \n",
    "#     g=g(:); % make gradient a vector for minFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex_softmax_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=np.empty((784,train_num))\n",
    "test_X=np.empty((784,test_num))\n",
    "for i in range(train_num):\n",
    "    train_X[:,i]=train_images[i].reshape(784)\n",
    "for i in range(test_num):\n",
    "    test_X[:,i]=test_images[i].reshape(784)\n",
    "cols1=train_X.shape[1]\n",
    "append1=np.ones(shape=(1,cols1))\n",
    "cols2=test_X.shape[1]\n",
    "append2=np.ones(shape=(1,cols2))\n",
    "# % Include a row of 1s as an additional intercept feature.\n",
    "# % Add row of 1s to the dataset to act as an intercept term.\n",
    "train_X = np.insert(train_X,0,values=append1)\n",
    "test_X = np.insert(test_X,0,values=append2)\n",
    "train_y=train_labels\n",
    "test_y=test_labels\n",
    "#% Training set dimensions\n",
    "m=train_X.shape[1]\n",
    "n=train_X.shape[0]\n",
    "theta = init_weights(k,n);\n",
    "def ex_softmax_reg(theta,X,y,lr,epoches):\n",
    "    res_theta=theta\n",
    "    costmin=10000\n",
    "    for i in range(epoches):\n",
    "        f,g=softmax_reg(theta,X,y)\n",
    "        print(\"the cost is %f\",f)\n",
    "        theta-=lr*g\n",
    "        if costmin>f:\n",
    "            costmin=f\n",
    "            res_theta=theta\n",
    "    return res_theta\n",
    "lr=0.05\n",
    "epoches=1000\n",
    "costmin=10000   \n",
    "res_theta=ex_softmax_reg(theta,train_X,train_y,lr,epoches)\n",
    "def classifier(theta,X):\n",
    "    re=np.dot(theta,X)\n",
    "    print(re)\n",
    "    re1=np.exp(re)\n",
    "    re2=np.sum(re1,axis=0)\n",
    "    re3=re1/re2\n",
    "    return re3\n",
    "def init_weights(k,n_features):\n",
    "    # 初始化参数\n",
    "    # 参数范围(-1/sqrt(N), 1/sqrt(N))\n",
    "    limit = np.sqrt(1/n_features)\n",
    "    w = np.random.uniform(-limit, limit, (k,n_features))\n",
    "    return w\n",
    "def binary_classifier_accuracy(theta,X,y):\n",
    "    predict_score=classifier(theta,X)\n",
    "    predicted_y=np.argmax(predict_score,axis=0)\n",
    "    lens=len(y)\n",
    "    acu=np.sum(predicted_y==y)/lens\n",
    "    return acu\n",
    "accuracy = binary_classifier_accuracy(res_theta,train_X,train_y);\n",
    "print('Training accuracy: %2.1f%\\n', 100*accuracy);\n",
    "\n",
    "#% Print out accuracy on the test set.\n",
    "accuracy = binary_classifier_accuracy(res_theta,test_X,test_y);\n",
    "print('Test accuracy: %2.1f%\\n', 100*accuracy);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "# n:特征数，k:标签数\n",
    "n = X.shape[1]\n",
    "label = np.unique(y)\n",
    "k = len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(k,n_features):\n",
    "    # 初始化参数\n",
    "    # 参数范围(-1/sqrt(N), 1/sqrt(N))\n",
    "    limit = np.sqrt(1/n_features)\n",
    "    w = np.random.uniform(-limit, limit, (k,n_features))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 112) (5, 23)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=0.75, test_size=0.15, random_state=1)\n",
    "X_train=X_train.T\n",
    "X_train=np.insert(X_train, 0, 1, axis=0)  # 添加一行\n",
    "X_test=X_test.T\n",
    "X_test=np.insert(X_test, 0, 1, axis=0)  # 添加一行\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=X_train.shape[1]\n",
    "n=X_train.shape[0]\n",
    "theta = init_weights(k,n);\n",
    "print(theta)\n",
    "epoches=1000\n",
    "lr=0.000001 \n",
    "res_theta=ex_softmax_reg(theta,X_train,Y_train,lr,epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "# n:特征数，k:标签数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax=Softmax()\n",
    "softmax.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "class Softmax(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.learning_step = 0.000001           # 学习速率\n",
    "        self.max_iteration = 100000             # 最大迭代次数\n",
    "        self.weight_lambda = 0.01               # 衰退权重\n",
    " \n",
    "    def cal_e(self,x,l):\n",
    "#         '''\n",
    "#         计算博客中的公式3\n",
    "#         '''\n",
    " \n",
    "        theta_l = self.w[l]\n",
    "        product = np.dot(theta_l,x)\n",
    " \n",
    "        return math.exp(product)\n",
    " \n",
    "    def cal_probability(self,x,j):\n",
    "#         '''\n",
    "#         计算博客中的公式2\n",
    "#         '''\n",
    " \n",
    "        molecule = self.cal_e(x,j)\n",
    "        denominator = sum([self.cal_e(x,i) for i in range(self.k)])\n",
    " \n",
    "        return molecule/denominator\n",
    " \n",
    " \n",
    "    def cal_partial_derivative(self,x,y,j):\n",
    "#         '''\n",
    "#         计算博客中的公式1\n",
    "#         '''\n",
    " \n",
    "        first = int(y==j)                           # 计算示性函数\n",
    "        second = self.cal_probability(x,j)          # 计算后面那个概率\n",
    " \n",
    "        return -x*(first-second) + self.weight_lambda*self.w[j]\n",
    " \n",
    "    def predict_(self, x):\n",
    "        result = np.dot(self.w,x)\n",
    "        row, column = result.shape\n",
    " \n",
    "        # 找最大值所在的列\n",
    "        _positon = np.argmax(result)\n",
    "        m, n = divmod(_positon, column)\n",
    " \n",
    "        return m\n",
    " \n",
    "    def train(self, features, labels):\n",
    "        self.k = len(set(labels))\n",
    " \n",
    "        self.w = np.zeros((self.k,len(features[0])+1))\n",
    "        time = 0\n",
    " \n",
    "        while time < self.max_iteration:\n",
    "            print('loop %d' % time)\n",
    "            time += 1\n",
    "            index = random.randint(0, len(labels) - 1)\n",
    " \n",
    "            x = features[index]#选一个样本出来，features为nXm矩阵，n为样本个数，m为特征数\n",
    "            y = labels[index]\n",
    " \n",
    "            x = list(x)\n",
    "            x.append(1.0)\n",
    "            x = np.array(x)\n",
    " \n",
    "            derivatives = [self.cal_partial_derivative(x,y,j) for j in range(self.k)]\n",
    " \n",
    "            for j in range(self.k):\n",
    "                self.w[j] -= self.learning_step * derivatives[j]\n",
    " \n",
    "    def predict(self,features):\n",
    "        labels = []\n",
    "        for feature in features:\n",
    "            x = list(feature)\n",
    "            x.append(1)\n",
    " \n",
    "            x = np.matrix(x)\n",
    "            x = np.transpose(x)\n",
    " \n",
    "            labels.append(self.predict_(x))\n",
    "        return labels   \n",
    "    def classifier(self,X):\n",
    "        X1=X.copy()\n",
    "        m=X.shape[1]\n",
    "        b=np.ones(X.shape[0])\n",
    "        X1=np.insert(X1,m,values=b,axis=1)\n",
    "        re=np.dot(X1,self.w.T)\n",
    "        print(re)\n",
    "        re1=np.exp(re)\n",
    "        re2=np.sum(re1,axis=1).reshape(X.shape[0],1)\n",
    "        re3=re1/re2\n",
    "        return re3\n",
    "    def binary_classifier_accuracy(self,X,y):\n",
    "        predict_score=self.classifier(X)\n",
    "        predicted_y=np.argmax(predict_score,axis=1)\n",
    "        lens=len(y)\n",
    "        acu=np.sum(predicted_y==y)/lens\n",
    "        return acu\n",
    "# if __name__ == '__main__':\n",
    " \n",
    "#     print('Start read data')\n",
    " \n",
    "#     time_1 = time.time()\n",
    " \n",
    "#     raw_data = pd.read_csv('../data/train.csv', header=0)\n",
    "#     data = raw_data.values\n",
    " \n",
    "#     imgs = data[0::, 1::]\n",
    "#     labels = data[::, 0]\n",
    " \n",
    "#     # 选取 2/3 数据作为训练集， 1/3 数据作为测试集\n",
    "#     train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "#         imgs, labels, test_size=0.33, random_state=23323)\n",
    "#     # print train_features.shape\n",
    "#     # print train_features.shape\n",
    " \n",
    "#     time_2 = time.time()\n",
    "#     print('read data cost '+ str(time_2 - time_1)+' second')\n",
    " \n",
    "#     print('Start training')\n",
    "#     p = Softmax()\n",
    "#     p.train(train_features, train_labels)\n",
    " \n",
    "#     time_3 = time.time()\n",
    "#     print('training cost '+ str(time_3 - time_2)+' second')\n",
    " \n",
    "#     print('Start predicting')\n",
    "#     test_predict = p.predict(test_features)\n",
    "#     time_4 = time.time()\n",
    "#     print('predicting cost ' + str(time_4 - time_3) +' second')\n",
    " \n",
    "#     score = accuracy_score(test_labels, test_predict)\n",
    "#     print(\"The accruacy socre is \" + str(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
